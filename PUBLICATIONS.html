<!DOCTYPE html>

<html class="csstransforms csstransforms3d csstransitions">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>YU LIU</title>
  <link href="./PUBLICATIONS_files/bootstrap.css" rel="stylesheet" type="text/css" media="all">
  <link href="./PUBLICATIONS_files/style.css" rel="stylesheet" type="text/css" media="all">
  <link href="./PUBLICATIONS_files/prettyPhoto.css" rel="stylesheet" type="text/css" media="all">
  <link href="./PUBLICATIONS_files/css" rel="stylesheet" type="text/css">


  <style>
    .header {
      background: #0B2D64;
      /* 你想换色就改这里 */
    }
  </style>

</head>

<body>
  <!---start-wrap--->
  <!---start-header--->
  <div class="header">
    <div class="wrap">
      <!---start-logo--->
      <div class="logo">
        <a href="https://yuliu316316.github.io/">YU LIU</a>
      </div>
      <!---End-logo--->
      <!---start-top-nav--->
      <div class="top-nav">
        <ul>
          <li id="Home" onclick="func(&#39;Me&#39;)"><a href="https://yuliu316316.github.io/" class="scroll">Home</a>
          </li>
          <li id="Profile" onclick="func(&#39;Profile&#39;)"><a href="https://yuliu316316.github.io/PROFILE.html"
              class="scroll">Profile</a></li>

          <li id="Profile" onclick="func(&#39;Profile&#39;)"><a href="https://yuliu316316.github.io/PUBLICATIONS.html"
              class="scroll">Publications</a></li>
          <li id="MVPLab" onclick="func(&#39;MVP Lab&#39;)"><a href="https://yuliu316316.github.io/GROUP.html"
              class="scroll">Group</a></li>
          <li id="Profile" onclick="func(&#39;Profile&#39;)"><a href="https://yuliu316316.github.io/CONTACT.html"
              class="scroll">CONTACT</a></li>
          <!--<li id="Profile" onclick="func(&#39;Profile&#39;)"><a href="https://yuliu316316.github.io/Chinese.html"
              class="scroll">中文简介</a></li>-->
        </ul>
      </div>
      <div class="clear"> </div>
      <!---End-top-nav--->
    </div>
  </div>

  <script>
    var lastname = 'Me';
    function func(name) {
      var div = document.getElementById(name);
      div.className = 'active';
      div = document.getElementById(lastname);
      div.className = '';
      lastname = name;
    }
    function coming_soon() {
      alert("Coming soon.");
    }
  </script>

  <!---End-wrap--->
  <div class="content">
    <div class="grid3">
      <div class="grid3-content">





        <!--COPYRIGHT: The copyright of the following materials belongs to corresponding publishers. 
They are provided only for research and educational use that does not conflict to the interests of the publishers.
-->

        <hr>
        <!-- 			
<h4><b>Pre-print:</b></h4>
<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">
				

  </ul>
  <hr />-->


        <h3><b>Publications (* indicates the corresponding author):</b></h3>

        <!-- Note: * indicates the corresponding author. -->

        <br>
        <h4><b>2026:</b></h4>
        <ol class="n8H08c BKnRcf " style="list-style-type: decimal; margin-left: 0; margin-right: 0; padding: 0;">
         
          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Yu Shi, <strong>Yu Liu</strong>*, Zhong-Cheng Wu, Juan Cheng, Huafeng Li, Xun Chen,
                Degradation-Robust Fusion: An Efficient Degradation-Aware Diffusion Framework for Multimodal Image Fusion in Arbitrary Degradation Scenarios,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, accepted, 2026.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Zengyi Yang, <strong>Yu Liu</strong>*, Juan Cheng, Zhiqin Zhu, Yafei Zhang, Huafeng Li*,
                Customized Fusion: A Closed-Loop Dynamic Network for Adaptive Multi-Task-Aware Infrared-Visible Image Fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, accepted, 2026.</span>
            </p>
          </li>
          
          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Shuai Yuan, <strong>Yu Liu</strong>*, Xiaopei Zhang, Xiang Yan*, Hanlin Qin, Naveed Akhtar, 
                SP-KAN: Sparse-sine perception Kolmogorov–Arnold networks for infrared small target detection, 
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                ISPRS Journal of Photogrammetry and Remote Sensing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 234, pp. 1-19, 2026.</span>
                (<a class="XqQF9c" href="https://github.com/xdFai/SP-KAN" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>
          
          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Jiejiang Yu, <strong>Yu Liu</strong>*, 
                DPFR: Semi-supervised gland segmentation via density perturbation and feature recalibration, 
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Medical Image Analysis</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 110, p. 103962, 2026.</span>
                (<a class="XqQF9c" href="https://github.com/Methow0/DPFR" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Zhiqin Zhu, Yang Yang, Guanqiu Qi*, Shuang Li*, Huafeng Li, <strong>Yu Liu</strong>*, 
                Seeing Clearly, Detecting Precisely: Perceptual Enhancement and Focus Calibration for Small Object Detection, 
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Neural Networks and Learning Systems</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, in press, 2026.</span>
            </p>
          </li>


          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Yida Chen, Yafei Zhang, Huafeng Li,Zhengtao Yu, <strong>Yu Liu</strong>, 
                Joint Multi-View Embedding with Progressive Multi-Scale Alignment for Unaligned Infrared-Visible Image Fusion, 
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Information Fusion</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 128, p. 103960, 2026.</span>
                (<a class="XqQF9c" href="https://github.com/yidamyth/ME-PMA" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Huafeng Li, Jiaqi Fang, Yafei Zhang, <strong>Yu Liu</strong>, 
                Infrared-assisted single-stage framework for joint restoration and fusion of visible and infrared images under hazy conditions, 
              </span><span class="C9DxTc "style="color: #000000; font-style: italic; font-variant: normal;">
                Pattern Recognition</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 175, p. 113074, 2026.</span>
                (<a class="XqQF9c" href="https://github.com/fangjiaqi0909/IASSF" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Jiajie Li, Juan Cheng, Rencheng Song, <strong>Yu Liu</strong>, 
                LST-rPPG: A Long-range Spatio-temporal Model for High-accuracy Heart Rate Variability Measurement, 
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Expert Systems with Applications</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 298, p. 129526, 2026.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Xifeng Hu, Yankun Cao, Weifeng Hu, Shanshan Wang, Subhas Chandra Mukhopadhyay, <strong>Yu Liu</strong>, Huafeng Li, Yujun Li, Qing Cai, Zhi Liu, 
                Expert knowledge-guided multi-granularity multi-scale fusion for weakly-supervised histological segmentation, 
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Information Fusion</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 125, p. 103432, 2026.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Zeyu Xiao, Zhuoyuan Li, Yang Zhao, <strong>Yu Liu</strong>, Zhao Zhang, Wei Jia, 
                Learning Dual Modality Interactions for Event-based Motion Deblurring,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Multimedia</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, in press, 2026.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Yafei Zhang, Shuaitian Song, Huafeng Li, Shujuan Wang, <strong>Yu Liu</strong>,
                Adaptive Dynamic Dehazing via Instruction-Driven and Task-Feedback Closed-Loop Optimization for Diverse Downstream Task Adaptation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                AAAI Conference on Artificial Intelligence (AAAI)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, accepted, 2026.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Yafei Zhang, Meng Ma, Huafeng Li, <strong>Yu Liu</strong>,
                Missing No More: Dictionary-Guided Cross-Modal Image Fusion under Missing Infrared,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, accepted, 2026.</span>
            </p>
          </li>



        

          <br>
          <h4><b>2025:</b></h4>
        

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Huafeng Li, Zengyi Yang, Yafei Zhang, Wei Jia, Zhengtao Yu, <strong>Yu Liu</strong>*,
                MulFS-CAP: Multimodal Fusion-supervised Cross-modality Alignment Perception for Unregistered Infrared-visible Image Fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 47, pp. 3673-3690, 2025.</span>
              </span>(<strong>ESI Hot Paper, ESI Highly Cited Paper, Google Scholar Citations: 80+</strong>)
              (<a class="XqQF9c" href="https://github.com/YR0211/MulFS-CAP" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Yu Shi, <strong>Yu Liu</strong>*, Juan Cheng, Z. Jane Wang, Xun Chen,
                VDMUFusion: A Versatile Diffusion Model-Based Unsupervised Framework for Image Fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Image Processing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 34, pp. 441-454, 2025.</span>
              </span>(<strong>ESI Highly Cited Paper, Google Scholar Citations: 50+</strong>)
              (<a class="XqQF9c" href="https://github.com/yuliu316316/VDMUFusion" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Yan Feng, Juan Cheng, Haolin Zhan, Zhiqin Zhu, 
                MambaDiff: Mamba-Enhanced Diffusion Model for 3D Medical Image Segmentation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Image Processing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 34, pp. 5761-5775, 2025.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Zhiqin Zhu, Zimeng Zhang, Guanqiu Qi, Yuanyuan Li, Pan Yang, <strong>Yu Liu</strong>*,
                Probability Map-Guided Network for 3D Volumetric Medical Image Segmentation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Image Processing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 34, pp. 7222-7234, 2025.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Yu Shi, <strong>Yu Liu</strong>*, Juan Cheng, Huafeng Li, Xun Chen,
                Semantic-Guided Diffusion Sampling: A Generalized Strategy for Enhancing Object Segmentation Oriented Multimodal Image Fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Journal of Selected Topics in Signal Processing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 19, pp. 1603-1616, 2025.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Kecheng Zheng, Juan Cheng, <strong>Yu Liu</strong>*,
                Unfolding coupled convolutional sparse representation for multi-focus image fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Information Fusion</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 118, p. 102974, 2025.</span>
              (<a class="XqQF9c" href="https://github.com/yuliu316316/CCSR-Net-Fusion" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Kai Cui, Jia Li*, <strong>Yu Liu</strong>*, Xuesong Zhang, Zhenzhen Hu, Meng Wang,
                PhysioSync: Temporal and Cross-Modal Contrastive Learning Inspired by Physiological Synchronization for EEG-Based Emotion Recognition,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Computational Social Systems</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, in press, 2025.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Zhiqin Zhu, Zhandong Wang, Guanqiu Qi, Yuanxiang Zhao, <strong>Yu Liu</strong>*,
                Visually Stabilized Mamba U-Shaped Network With Strong Inductive Bias for 3-D Brain Tumor Segmentation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Instrumentation and Measurement</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 74, p. 2518511, 2025.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Dingyi Li, Yang Zhang, <strong>Yu Liu</strong>*,
                Lightweight Efficient Rate-Adaptive Network for Compression-Aware Image Rescaling,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Signal Processing Letters</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 32, pp. 691-695, 2025.</span>
              (<a class="XqQF9c" href="https://github.com/5ofwind/LERAN" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Xinyue Hu, Yu Shi, <strong>Yu Liu</strong>*,
                Classification of Alzheimer's disease using multimodal brain imaging via DenseNet and Mamba,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Journal of Image and Graphics</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 30, pp. 3230-3241, 2025. (in Chinese)</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Xiaoshuai Hao, Guanqun Liu, Yuting Zhao, Yuheng Ji, Mengchuan Wei, Haimei Zhao, Lingdong Kong, Rong Yin*, <strong>Yu Liu</strong>*,
                MSC-Bench: Benchmarking and Analyzing Multi-Sensor Corruption for Driving Perception,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE International Conference on Multimedia &amp; Expo (ICME)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, Nantes, France, Jun. 30–Jul. 4, 2025, pp. 1-6.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Juan Cheng, Xiwen Luo, Xiaowei Wu, Rencheng Song, <strong>Yu Liu</strong>,
                Video-based Instantaneous Heart Rate Measurement with Enhanced Time-Frequency Representations,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Multimedia</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, in press, 2025.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Zengyi Yang, Yafei Zhang, Huafeng Li, <strong>Yu Liu</strong>,
                Instruction-driven fusion of Infrared–visible images: Tailoring for diverse downstream tasks,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Information Fusion</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 121, p. 103148, 2025.</span>
              (<a class="XqQF9c" href="https://github.com/YR0211/IDF-TDDT" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Xiaoshuai Hao, Yunfeng Diao, Mengchuan Wei, Yifan Yang, Peng Hao, Rong Yin, Hui Zhang, Weiming Li, Shu Zhao, <strong>Yu Liu</strong>,
                MapFusion: A novel BEV feature fusion network for multi-modal map construction,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Information Fusion</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 119, p. 103018, 2025.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Huafeng Li, Ming Yuan, Jinxing Li, <strong>Yu Liu</strong>, Guangming Lu, Yong Xu, Zhengtao Yu, David Zhang,
                Focus Affinity Perception and Super-Resolution Embedding for Multifocus Image Fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Neural Networks and Learning Systems</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 25, pp. 4311-4325, 2025.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Dayong Su, Yafei Zhang, Huafeng Li, Jingxin Li, <strong>Yu Liu</strong>,
                UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE/CVF International Conference on Computer Vision (ICCV)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, pp. 14238-14247, 2025.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Dasong Li, …, <strong>Yu Liu</strong>, et al.,
                VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods and Results,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE/CVF International Conference on Computer Vision (ICCV) Workshop</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, pp. 3391-3401, 2025.</span>
            </p>
          </li>
        


          <br>
          <h4><b>2024:</b></h4>
        

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Zhengzheng Qi, Juan Cheng, Xun Chen,
                Rethinking the Effectiveness of Objective Evaluation Metrics in Multi-focus Image Fusion: A Statistic-based Approach,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 46, pp. 5806-5819, 2024.</span>
              </span>(<strong>ESI Highly Cited Paper, Google Scholar Citations: 60+</strong>)
              (<a class="XqQF9c" href="https://github.com/yuliu316316/MFIF-Metrics" target="_blank" style="color: inherit; text-decoration: none;"><strong><font color="#034EA1">Code</font></strong></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Huafeng Li, Junyu Liu, Yafei Zhang, <strong>Yu Liu</strong>*,
                A Deep Learning Framework for Infrared and Visible Image Fusion without Strict Registration,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                International Journal of Computer Vision</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 132, pp. 1625-1644, 2024.</span>
              (<a class="XqQF9c" href="https://github.com/yuliu316316/IVF-WoReg" target="_blank" style="color: inherit; text-decoration: none;"><strong><font color="#034EA1">Code</font></strong></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Chen Yu, Juan Cheng, Z. Jane Wang, Xun Chen,
                MM-Net: A MixFormer-Based Multi-Scale Network for Anatomical and Functional Image Fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Image Processing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 33, pp. 2197-2212, 2024.</span>
              (<a class="XqQF9c" href="https://github.com/yuliu316316/MM-Net-Fusion" target="_blank" style="color: inherit; text-decoration: none;"><strong><font color="#034EA1">Code</font></strong></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Yize Ma, Zhiqin Zhu, Juan Cheng, Xun Chen,
                TransSea: Hybrid CNN-Transformer With Semantic Awareness for 3-D Brain Tumor Segmentation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Instrumentation and Measurement</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 73, p. 2521316, 2024.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Dingyi Li, <strong>Yu Liu</strong>*, Zengfu Wang, Jian Yang,
                Video Rescaling with Recurrent Diffusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Circuits and Systems for Video Technology</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 34, pp. 9386-9399, 2024.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Yuhang Chen, Aiping Liu, <strong>Yu Liu</strong>*, Zhiyang He, Cong Liu, Xun Chen*,
                Multi-Dimensional Medical Image Fusion With Complex Sparse Representation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Biomedical Engineering</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 71, pp. 2728-2739, 2024.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Zhiqin Zhu, Ziyu Wang, Guanqiu Qi, Neal Mazur, Pan Yang, <strong>Yu Liu</strong>*,
                Brain tumor segmentation in MRI with multi-modality spatial information enhancement and boundary shape correction,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Pattern Recognition</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 153, p. 110553, 2024.</span>
              (<strong>ESI Hot Paper, ESI Highly Cited Paper, Google Scholar Citations: 100+</strong>)
            </p>
          </li>
        
        <!--
          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Zhiqin Zhu, Mengwei Sun, Guanqiu Qi, Yuanyuan Li, Xinbo Gao, <strong>Yu Liu</strong>*,
                Sparse Dynamic Volume TransUNet with Multi-level Edge Fusion for Brain Tumor Segmentation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Computers in Biology and Medicine</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 172, p. 108284, 2024.</span>
            </p>
          </li>
        -->

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Juan Cheng, Chenchu Yin, Rencheng Song, Jing Fu, <strong>Yu Liu</strong>*,
                Facial video-based heart rate measurement against irregular motion artifacts,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Journal of Image and Graphics</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 29, pp. 2024-2034, 2024. (in Chinese)</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Xiaowen Zhang, Aiping Liu, Gang Yang, <strong>Yu Liu</strong>, Xun Chen,
                SIMFusion: A semantic information-guided modality-specific fusion network for MR Images,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Information Fusion</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 112, p. 102560, 2024.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Wei Tang, Fazhi He, <strong>Yu Liu</strong>,
                ITFuse: An interactive transformer for infrared and visible image fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Pattern Recognition</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 156, p. 110822, 2024.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Ting Lv, Chuanming Ji, Hong Jiang, <strong>Yu Liu</strong>,
                HF2TNet: A Hierarchical Fusion Two-stage Training Network for Infrared and Visible Image Fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Signal Processing Letters</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 31, pp. 3164-3168, 2024.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Jing Zhang, Aiping Liu, <strong>Yu Liu</strong>, Bensheng Qiu, Qingguo Xie, Xun Chen,
                LeGFusion: Locally-enhanced Global Learning for Multi-Modal Image Fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Sensors Journal</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 24, pp. 12806-12818, 2024.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Kai Shi, Aiping Liu, Jing Zhang, <strong>Yu Liu</strong>, Xun Chen,
                Medical Image Fusion Based on Multi-level Bidirectional Feature Interaction Network,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Sensors Journal</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 24, pp. 19428-19441, 2024.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Yuhang Chen, <strong>Yu Liu</strong>, Rabab K. Ward, Xun Chen,
                Multi-focus Image Fusion With Complex Sparse Representation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Sensors Journal</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 24, pp. 34744-34755, 2024.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Jinbao Wei, Gang Yang, Zhijie Wang, <strong>Yu Liu</strong>, Aiping Liu, Xun Chen,
                Misalignment-resistant deep unfolding network for multi-modal MRI super-resolution and reconstruction,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Knowledge-Based Systems</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 296, p. 111866, 2024.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Hongyu Zhao, Chang Li, <strong>Yu Liu</strong>, Juan Cheng, Rencheng Song, Xun Chen,
                EEG emotion recognition based on source-free domain adaptation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Chinese Journal of Biomedical Engineering</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 43, pp. 129-142, 2024. (in Chinese)</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Meiyuan Luo, Chao Zhang, <strong>Yu Liu</strong>,
                An Efficient Lightweight Network for Breast Cancer Segmentation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                2nd International Conference on Machine Vision, Image Processing &amp; Imaging Technology (MVIPIT)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, pp. 155-160, 2024.</span>
            </p>
          </li>



        

          <br>
          <h4><b>2023:</b></h4>
        



          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Haihang Li, Juan Cheng, Xun Chen,
                MSCAF-Net: A General Framework for Camouflaged Object Detection via Learning Multi-Scale Context-Aware Features,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Circuits and Systems for Video Technology</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 33, pp. 4934-4947, 2023.</span>
              (<strong>ESI Hot Paper, ESI Highly Cited Paper, Google Scholar Citations: 100+</strong>)
              (<a class="XqQF9c" href="https://github.com/yuliu316316/MSCAF-COD" target="_blank" style="color: inherit; text-decoration: none;"><strong><font color="#034EA1">Code</font></strong></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Zhiqin Zhu, Xianyu He, Guanqiu Qi, Yuanyuan Li, Baisen Cong, <strong>Yu Liu</strong>*,
                Brain tumor segmentation based on the fusion of deep semantics and edge information in multimodal MRI,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Information Fusion</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 91, pp. 376-387, 2023.</span>
              (<strong>ESI Hot Paper, ESI Highly Cited Paper, Google Scholar Citations: 500+</strong>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Yi Wei, Chang Li, Juan Cheng, Rencheng Song, Xun Chen,
                Bi-CapsNet: A Binary Capsule Network for EEG-based Emotion Recognition,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Journal of Biomedical and Health Informatics</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 27, pp. 1319-1330, 2023.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Zhigang Yang, Juan Cheng, Xun Chen,
                Multi-Exposure Image Fusion via Multi-scale and Context-aware Feature Learning,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Signal Processing Letters</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 30, pp. 100-104, 2023.</span>
              (<a class="XqQF9c" href="https://github.com/yuliu316316/MSCA-MEF" target="_blank" style="color: inherit; text-decoration: none;"><strong><font color="#034EA1">Code</font></strong></a>)
            </p>
          </li>

        <!--
          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Yi Wei, <strong>Yu Liu</strong>*, Chang Li, Juan Cheng, Rencheng Song, Xun Chen,
                TC-Net: A Transformer Capsule Network for EEG-based Emotion Recognition,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Computers in Biology and Medicine</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 152, p. 106463, 2023.</span>
            </p>
          </li>
        -->

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Wenyu Zhu, Juan Cheng, Xun Chen,
                Multi-modal MR image super-resolution with residual dense attention network,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Journal of Image and Graphics</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 28, pp. 248-259, 2023. (in Chinese)</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Lei Wang, Zhengzheng Qi, <strong>Yu Liu</strong>*,
                The review of multi-focus image fusion methods based on deep learning,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Journal of Image and Graphics</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 28, pp. 80-101, 2023. (in Chinese)</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Kecheng Zheng, Juan Cheng, <strong>Yu Liu</strong>*,
                CCSR-Net: Unfolding Coupled Convolutional Sparse Representation for Multi-focus Image Fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">6th Chinese Conference on Pattern Recognition and Computer Vision (PRCV)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, Xiamen, China, Oct. 13-15, 2023, LNCS 14434, pp. 285-297.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Wei Tang, Fazhi He, <strong>Yu Liu</strong>,
                YDTR: Infrared and Visible Image Fusion via Y-shape Dynamic Transformer,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Multimedia</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 25, pp. 5413-5428, 2023.</span>
              (<strong>ESI Hot Paper, ESI Highly Cited Paper, Google Scholar Citations: 400+</strong>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Wei Tang, Fazhi He, <strong>Yu Liu</strong>, Yansong Duan, Tongzhen Si,
                DATFuse: Infrared and Visible Image Fusion via Dual Attention Transformer,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Circuits and Systems for Video Technology</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 33, pp. 3159-3172, 2023.</span>
              (<strong>ESI Hot Paper, ESI Highly Cited Paper, Google Scholar Citations: 400+</strong>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Wei Tang, Fazhi He, <strong>Yu Liu</strong>,
                TCCFusion: An Infrared and Visible Image Fusion Method based on Transformer and Cross Correlation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Pattern Recognition</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 137, p. 109295, 2023.</span>
              (<strong>ESI Highly Cited Paper, Google Scholar Citations: 100+</strong>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Juan Cheng, Runqing Liu, Jiajie Li, Rencheng Song, <strong>Yu Liu</strong>, Xun Chen,
                Motion-robust Respiratory Rate Estimation From Camera Videos via Fusing Pixel Movement and Pixel Intensity Information,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Instrumentation and Measurement</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 72, p. 4008611, 2023.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Juan Cheng, Bicheng Yue, Rencheng Song, <strong>Yu Liu</strong>, Chang Li, Xun Chen,
                Motion-robust anterior-posterior imaging ballistocardiography for non-contact heart rate measurements,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Biomedical Signal Processing and Control</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 86, p. 105307, 2023.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Xietian Wang, Aiping Liu, Le Wu, Chang Li, <strong>Yu Liu</strong>, Xun Chen,
                A Generalized Zero-shot Learning Scheme for SSVEP-Based BCI System,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Neural Systems and Rehabilitation Engineering</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 31, pp. 863-874, 2023.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Jing Zhang, <strong>Yu Liu</strong>, Aiping Liu, Qingguo Xie, Rabab Ward, Z. Jane Wang, Xun Chen,
                Multi-modal Image Fusion via Self-supervised Transformer,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Sensors Journal</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 23, pp. 9796-9807, 2023.</span>
            </p>
          </li>

        <!--
          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Muhua Xu, Juan Cheng, Chang Li, <strong>Yu Liu</strong>, Xun Chen,
                Spatio-temporal deep forest for emotion recognition based on facial electromyography signals,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Computers in Biology and Medicine</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 156, p. 106689, 2023.</span>
            </p>
          </li>
        -->

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Chang Li, Zhongzhen Zhang, Rencheng Song, Juan Cheng, <strong>Yu Liu</strong>, Xun Chen,
                EEG-based Emotion Recognition via Neural Architecture Search,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Affective Computing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 14, pp. 957-968, 2023.</span>
              (<strong>ESI Highly Cited Paper, Google Scholar Citations: 80+</strong>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Wei Tao, Chang Li, Rencheng Song, Juan Cheng, <strong>Yu Liu</strong>, Feng Wan, Xun Chen,
                EEG-based emotion recognition via channel-wise attention and self attention,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Affective Computing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 14, pp. 382-393, 2023.</span>
              (<strong>ESI Hot Paper, ESI Highly Cited Paper, Google Scholar Citations: 600+</strong>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Chang Li, Zhongzhen Zhang, Xiaodong Zhang, Guoning Huang, <strong>Yu Liu</strong>, Xun Chen,
                EEG-based Emotion Recognition via Transformer Neural Architecture Search,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Industrial Informatics</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 19, pp. 6016-6025, 2023.</span>
              (<strong>ESI Highly Cited Paper, Google Scholar Citations: 100+</strong>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Yuanyuan Li, Ziyu Wang, Li Yin, Zhiqin Zhu, Guanqiu Qi, <strong>Yu Liu</strong>,
                X-Net: A dual encoding-decoding method in medical image segmentation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                The Visual Computer</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 39, pp. 2223-2233, 2023.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Juan Cheng, Mengying Wu, Rencheng Song, <strong>Yu Liu</strong>, Xun Chen,
                Motion-robust heart rate measurement from videos via combining model with blind source separation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Chinese Journal of Scientific Instrument</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 44, pp. 289-298, 2023. (in Chinese)</span>
            </p>
          </li>



        

          <br>
          <h4><b>2022:</b></h4>
        
          
          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Lei Wang, Huafeng Li, Xun Chen,
                Multi-focus image fusion with deep residual learning and focus property detection,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Information Fusion</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 86-87, pp. 1-16, 2022.</span>
              (<a class="XqQF9c" href="https://github.com/yuliu316316/DRL-FPD-MFIF" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Hao Zhao, Rencheng Song, Xudong Chen, Chang Li, Xun Chen,
                SOM-Net: Unrolling the Subspace-based Optimization for Solving Full-wave Inverse Scattering Problems,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Geoscience and Remote Sensing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 60, p. 2007715, 2022.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Yu Shi, Fuhao Mu, Juan Cheng, Xun Chen,
                Glioma Segmentation-Oriented Multi-modal MR Image Fusion with Adversarial Learning,
              </span><span class="C9DxTc "
                style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE/CAA Journal of Automatica Sinica</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 9, pp. 1528-1531, 2022.</span>
              (<a class="XqQF9c" href="https://github.com/yuliu316316/GS-MR-Fusion" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Yu Shi, Fuhao Mu, Juan Cheng, Chang Li, Xun Chen,
                Multimodal MRI Volumetric Data Fusion with Convolutional Neural Networks,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Instrumentation and Measurement</span><span class="C9DxTc "
                style="color: #000000; font-variant: normal;">, vol. 71, p. 4006015, 2022.</span>
              (<a class="XqQF9c" href="https://github.com/yuliu316316/3D-CNN-Fusion" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Fuhao Mu, Yu Shi, Xun Chen,
                SF-Net: A Multi-task Model for Brain Tumor Segmentation in Multimodal MRI via Image Fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Signal Processing Letters</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 29, pp. 1799-1803, 2022.</span>
              (<a class="XqQF9c" href="https://github.com/yuliu316316/SF-Net" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Fuhao Mu, Yu Shi, Juan Cheng, Chang Li, Xun Chen,
                Brain Tumor Segmentation in Multimodal MRI via Pixel-level and Feature-level Image Fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Frontiers in Neuroscience</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 16, p. 1000587, 2022.</span>
            </p>
          </li>


          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Rongrong Wu, Lu Tang, Ningning Song,
                U-Net for Mediastinal Lymph Node Segmentation in Bronchial Ultrasound Elastic Images,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Journal of Image and Graphics</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 21, pp. 2082-2091, 2022. (in Chinese)</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Juan Cheng, Chuya Zhang, <strong>Yu Liu</strong>*, Chang Li, Zhiqin Zhu, Xun Chen,
                Glioma segmentation based on feature selection of multi-modal MR images,
              </span><span class="C9DxTc "
                style="color: #000000; font-style: italic; font-variant: normal;">
                Chinese Journal of Biomedical Engineering</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 41, pp. 513-526, 2022. (in Chinese)</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Wei Tang, Fazhi He, <strong>Yu Liu</strong>, Yansong Duan,
                MATR: Multimodal Medical Image Fusion via Multiscale Adaptive Transformer,
              </span><span class="C9DxTc "
                style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Image Processing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 31, pp. 5134-5149, 2022.</span>
              (<strong>ESI Highly Cited Paper, Google Scholar Citations: 300+</strong>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Jing Zhang, Aiping Liu, Dan Wang, <strong>Yu Liu</strong>, Z. Jane Wang, Xun Chen,
                Transformer-based End-to-End Anatomical and Functional Image Fusion,
              </span><span class="C9DxTc "
                style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Instrumentation and Measurement</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 71, p. 5019711, 2022.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Chang Li, Xuejuan Lin, <strong>Yu Liu</strong>, Rencheng Song, Juan Cheng, Xun Chen,
                EEG-based Emotion Recognition via Efficient Convolutional Neural Network and Contrastive Learning,
              </span><span class="C9DxTc "
                style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Sensors Journal</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 22, pp. 19608-19619, 2022.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Yuhang Chen, Aiping Liu, <strong>Yu Liu</strong>, Ruobing Qian, Qingguo Xie, Xun Chen,
                Image Fusion with Sparse Representation: A Novel Local Contrast-Based Preprocessing Strategy,
              </span><span class="C9DxTc "
                style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Sensors Letters</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 6, p. 7001604, 2022.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Chang Li, Yimeng Hou, Rencheng Song, Juan Cheng, <strong>Yu Liu</strong>, Xun Chen,
                Multi-channel EEG-based emotion recognition in the presence of noisy labels,
              </span><span class="C9DxTc "
                style="color: #000000; font-style: italic; font-variant: normal;">
                Science China Information Sciences</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 65, p. 140405, 2022.</span>
            </p>
          </li>

        <!--
          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Chang Li, Bin Wang, Silin Zhang, <strong>Yu Liu</strong>, Rencheng Song, Juan Cheng, Xun Chen,
                Emotion recognition from EEG based on multi-task learning with capsule network and attention mechanism,
              </span><span class="C9DxTc "
                style="color: #000000; font-style: italic; font-variant: normal;">
                Computers in Biology and Medicine</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 143, p. 105303, 2022.</span>
              (<strong>ESI Highly Cited Paper</strong>)
            </p>
          </li>
        -->

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Chang Li, Chenhong Sui, Rencheng Song, Juan Cheng, <strong>Yu Liu</strong>, Xun Chen,
                Superpixel-based noise robust sparse unmixing of hyperspectral image,
              </span><span class="C9DxTc "
                style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Geoscience and Remote Sensing Letters</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 19, p. 6004405, 2022.</span>
            </p>
          </li>



          <br>
          <h4><b>2021:</b></h4>
        

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Lei Wang, Juan Cheng, Xun Chen,
                Multiscale feature interactive network for multifocus image fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Instrumentation and Measurement</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 70, p. 5019316, 2021.</span>
              (<a class="XqQF9c" href="https://github.com/yuliu316316/MSFIN-Fusion" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Wei Tang, <strong>Yu Liu</strong>*, Juan Cheng, Chang Li, Xun Chen,
                Green fluorescent protein and phase contrast image fusion via detail preserving cross network,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Computational Imaging</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 7, pp. 584-597, 2021.</span>
              (<a class="XqQF9c" href="https://github.com/yuliu316316/DPCN-Fusion" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Huafeng Li, Yueliang Cen, <strong>Yu Liu</strong>*, Xun Chen, Zhengtao Yu,
                Different input resolutions and arbitrary output resolution: A meta learning-based deep framework for infrared and visible image fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Image Processing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 30, pp. 4070-4083, 2021.</span>
              (<a class="XqQF9c" href="https://github.com/yuliu316316/MetaLearning-Fusion" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Rencheng Song, Qiao Zhou, <strong>Yu Liu</strong>*, Chang Li, Xun Chen,
                A convolutional sparsity regularization for solving inverse scattering problems,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Antennas and Wireless Propagation Letters</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 20, no. 12, pp. 2285-2289, 2021.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Xun Chen, Aiping Liu, Rabab. K. Ward, Z. Jane Wang,
                Recent advances in sparse representation based medical image fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Instrumentation &amp; Measurement Magazine</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 24, no. 2, pp. 45-53, 2021.</span>
              (<strong>Invited Paper</strong>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Wei Tang, Lei Wang, <strong>Yu Liu</strong>*,
                Green fluorescent protein and phase contrast image fusion via dual attention residual network,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE International Conference on Medical Imaging Physics and Engineering (ICMIPE)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, Hefei, China, Nov. 12-14, 2021, pp. 1-6.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Juan Cheng, Meiyao Chen, Chang Li, <strong>Yu Liu</strong>, Rencheng Song, Aiping Liu, Xun Chen,
                Emotion recognition from multi-channel EEG via deep forest,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Journal of Biomedical and Health Informatics</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 25, no. 2, pp. 453-464, 2021.</span>
              (<strong>ESI Highly Cited Paper, Google Scholar Citations: 200+</strong>)
            </p>
          </li>

        <!--
          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Juan Cheng, Yufei Xu, Rencheng Song, <strong>Yu Liu</strong>, Chang Li, Xun Chen,
                Prediction of arterial blood pressure waveforms from photoplethysmogram signals via fully convolutional neural networks,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Computers in Biology and Medicine</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 138, p. 104877, 2021.</span>
            </p>
          </li>
        -->

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Rencheng Song, Huan Chen, Juan Cheng, Chang Li, <strong>Yu Liu</strong>, Xun Chen,
                PulseGAN: Learning to generate realistic waveforms in remote photoplethysmography,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Journal of Biomedical and Health Informatics</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 25, no. 5, pp. 1373-1384, 2021.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Juan Cheng, Ping Wang, Rencheng Song, <strong>Yu Liu</strong>, Chang Li, Yong Liu, Xun Chen,
                Remote heart rate measurement from near-infrared videos based on joint blind source separation with delay-coordinate transformation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Instrumentation and Measurement</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 70, p. 5005313, 2021.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Rencheng Song, Jiji Li, Juan Cheng, Chang Li, <strong>Yu Liu</strong>, Xun Chen,
                Motion robust imaging ballistocardiography through a two-step canonical correlation analysis,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Instrumentation and Measurement</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 70, p. 4001710, 2021.</span>
            </p>
          </li>



        
          <br>
          <h4><b>2020 and earlier:</b></h4>
        
          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Lei Wang, Juan Cheng, Chang Li, Xun Chen,
                Multi-focus image fusion: A survey of the state of the art,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Information Fusion</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 64, pp. 71-91, 2020.</span>
              (<strong>Invited Paper, ESI Highly Cited Paper, Google Scholar Citations: 300+</strong>)
              (<a class="XqQF9c" href="https://github.com/yuliu316316/MFIF" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>A Benchmark for MFIF</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Chao Zhang, Chang Li, Juan Cheng, Yadong Zhang, Huiqin Xu, Tao Song, Liang Zhao, Xun Chen,
                A practical PET/CT data visualization method with dual-threshold PET colorization and image fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Computers in Biology and Medicine</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 126, p. 104050, 2020.</span>
              (<a class="XqQF9c" href="https://github.com/yuliu316316/Visualization" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Yufeng Ding, Chang Li, Juan Cheng, Rencheng Song, Feng Wan, Xun Chen,
                Multi-channel EEG-based emotion recognition via a multi-level features guided capsule network,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Computers in Biology and Medicine</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 123, p. 103927, 2020.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Wei Tang, <strong>Yu Liu</strong>*, Juan Cheng, Chang Li, Hu Peng, Xun Chen,
                A phase congruency-based green fluorescent protein and phase contrast image fusion method in nonsubsampled shearlet transform domain,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Microscopy Research and Technique</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 83, no. 10, pp. 1225-1234, 2020.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Xinle Wang, Haiyang Qian, Edward J. Ciaccio, Suzanne K. Lewis, Govind Bhagat, Peter H. Green, Shenghao Xu, Liang Huang, Rongke Gao*, <strong>Yu Liu</strong>*,
                Celaic disease diagnosis from videocapsle endoscopy images with residual learning and deep feature extraction,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Computer Methods and Programs in Biomedicine</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 187, p. 105236, 2020.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Yu Zhang, <strong>Yu Liu</strong>, Peng Sun, Han Yan, Xiaolin Zhao, Li Zhang,
                IFCNN: A general image fusion framework based on convolutional neural network,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Information Fusion</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 54, pp. 99-118, 2020.</span>
              (<strong>ESI Highly Cited Paper, Google Scholar Citations: 1500+</strong>)
              (<a class="XqQF9c" href="https://github.com/uzeful/IFCNN" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Mingyao Zheng, Guanqiu Qi, Zhiqing Zhu, Yuanyuan Li, Hongyan Wei, <strong>Yu Liu</strong>,
                Image dehazing by an artificial image fusion method based on adaptive structure decomposition,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Sensors Journal</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 20, no. 14, pp. 8062-8072, 2020.</span>
              (<strong>ESI Highly Cited Paper, Google Scholar Citations: 100+</strong>)
            </p>
          </li>

        <!--
          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Puhong Duan, Xudong Kang, Pedram Ghamisi, <strong>Yu Liu</strong>,
                Multilevel structure extraction-based multi-sensor data fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Remote Sensing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 12, no. 24, p. 4034, 2020.</span>
            </p>
          </li>
        -->

        <!--
          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Chang Li, <strong>Yu Liu</strong>, Juan Cheng, Rencheng Song, Jiayi Ma, Chenhong Sui, Xun Chen,
                Sparse unmixing of hyperspectral data with bandwise model,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Information Sciences</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 512, pp. 1424-1441, 2020.</span>
            </p>
          </li>
        -->

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Xuesong Wang, Chen Chen, Yuhu Cheng, Xun Chen, <strong>Yu Liu</strong>,
                Zero-shot learning based on deep weighted attribute prediction,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Systems, Man, and Cybernetics: Systems</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 50, no. 8, pp. 2948-2957, 2020.</span>
            </p>
          </li>



          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Ming Yin, Xiaoning Liu, <strong>Yu Liu</strong>*, Xun Chen,
                Medical image fusion with parameter-adaptive pulse coupled neural network in nonsubsampled shearlet transform domain,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Instrumentation and Measurement</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 68, no. 1, pp. 49-64, 2019.</span>
              (<strong>2020 IEEE TIM Andy Chi Best Paper Award, ESI Highly Cited Paper, Google Scholar Citations: 600+</strong>)
              (<a class="XqQF9c" href="https://github.com/yuliu316316/NSST-PAPCNN-Fusion" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Xun Chen, Rabab Ward, Z. Jane Wang,
                Medical image fusion via convolutional sparsity based morphological component analysis,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Signal Processing Letters</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 26, no. 3, pp. 485-489, 2019.</span>
              (<strong>ESI Highly Cited Paper, Google Scholar Citations: 300+</strong>)
              (<a class="XqQF9c" href="https://github.com/yuliu316316/CSMCA-Fusion" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Chao Zhang, Juan Cheng, Xun Chen, Z. Jane Wang,
                A multi-scale data fusion framework for bone age assessment with convolutional neural networks,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Computers in Biology and Medicine</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 108, pp. 161-173, 2019.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Dingyi Li, <strong>Yu Liu</strong>, Zengfu Wang,
                Video super-resolution using non-simultaneous fully recurrent convolutional network,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Image Processing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 28, no. 3, pp. 1342-1355, 2019.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Lei Ma, <strong>Yu Liu</strong>, Xueliang Zhang, Yuanxin Ye, Gaofei Yin, Brian Alan Johoson,
                Deep learning in remote sensing applications: A meta-analysis and review,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                ISPRS Journal of Photogrammetry and Remote Sensing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 152, pp. 166-177, 2019.</span>
              (<strong>ESI Hot Paper, ESI Highly Cited Paper, Google Scholar Citations: 2600+</strong>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Chang Li, Wei Tao, Juan Cheng, <strong>Yu Liu</strong>, Xun Chen,
                Robust multichannel EEG compressed sensing in the presence of mixed noise,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Sensors Journal</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 19, no. 22, pp. 10574-10583, 2019.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Xun Chen, Juan Cheng, Rencheng Song, <strong>Yu Liu</strong>, Rabab Ward, Z. Jane Wang,
                Video-Based Heart Rate Measurement: Recent Advances and Future Prospects,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Transactions on Instrumentation and Measurement</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 68, no. 10, pp. 3600-3615, 2019.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Xun Chen, Zengfu Wang, Z. Jane Wang, Rabab Ward, Xuesong Wang,
                Deep learning for pixel-level image fusion: recent advances and future prospects,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Information Fusion</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 42, pp. 158-173, 2018.</span>
              (<strong>Invited Paper, ESI Highly Cited Paper, Google Scholar Citations: 800+</strong>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Xun Chen, Juan Cheng, Hu Peng, Zengfu Wang,
                Infrared and visible image fusion with convolutional neural networks,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                International Journal of Wavelets, Multiresolution and Information Processing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 16, no. 3, 1850018: 1-20, 2018.</span>
              (<strong>ESI Highly Cited Paper, Google Scholar Citations: 500+</strong>)
              (<a class="XqQF9c" href="https://github.com/yuliu316316/CNN-IV-Fusion" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Xun Chen, Chao Zhang, <strong>Yu Liu</strong>*,
                Bone Age Assessment with X-Ray Images Based on Contourlet Motivated Deep Convolutional Networks,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                20th IEEE International Workshop on Multimedia Signal Processing (MMSP)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, Vancouver, Canada, Aug. 29-31, 2018, pp. 1-6.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Chang Li, <strong>Yu Liu</strong>, Juan Cheng, Rencheng Song, Hu Peng, Qiang Chen, Xun Chen,
                Hyperspectral unmixing with bandwise generalized bilinear model,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Remote Sensing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 2018, no. 10, paper ID 1600, pp. 1-10, 2018.</span>
            </p>
          </li>

        <!--
          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Juan Cheng, Fulin Wei, Chang Li, <strong>Yu Liu</strong>, Aiping Liu, Xun Chen,
                Position-independent gesture recognition using sEMG signals via canonical correlation analysis,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Computers in Biology and Medicine</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 103, pp. 44-54, 2018.</span>
            </p>
          </li>
        -->

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Xun Chen, Hu Peng, Zengfu Wang,
                Multi-focus image fusion with a deep convolutional neural network,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Information Fusion</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 36, pp. 191–207, 2017.</span>
              (<strong>ESI Highly Cited Paper, Google Scholar Citations: 1400+</strong>)
              (<a class="XqQF9c" href="https://github.com/yuliu316316/CNN-Fusion" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Xun Chen, Juan Cheng, Hu Peng,
                A medical image fusion method based on convolutional neural networks,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                20th International Conference on Information Fusion (ICIF)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, Xi'an, China, July 10-13, 2017, pp. 1070-1076.</span>
              (<strong>InCites Top 1%, Google Scholar Citations: 500+</strong>)
              (<a class="XqQF9c" href="https://github.com/yuliu316316/CNN-Medical-Fusion" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

        <!--
          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Xun Chen, Aiping Liu, Qiang Chen, <strong>Yu Liu</strong>, Liang Zou,
                Simultaneous Ocular and Muscle Artifact Removal From EEG Data by Exploiting Diverse Statistics,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Computers in Biology and Medicine</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 88, pp. 1-10, 2017.</span>
            </p>
          </li>
        -->

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Dingyi Li, <strong>Yu Liu</strong>, Zengfu Wang,
                Video super-resolution using motion compensation and residual bidirectional recurrent convolutional network,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                24th IEEE International Conference on Image Processing (ICIP)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, Beijing, China, Sep. 17-20, 2017, pp. 1642-1646.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Xun Chen, Rabab Ward, Z. Jane Wang,
                Image Fusion With Convolutional Sparse Representation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IEEE Signal Processing Letters</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 23, no. 12, pp. 1882-1886, 2016.</span>
              (<strong>ESI Highly Cited Paper, Google Scholar Citations: 1000+</strong>)
              (<a class="XqQF9c" href="https://github.com/yuliu316316/CSR-Fusion" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Shuping Liu, Yang Cao, Zengfu Wang,
                Automatic chessboard corner detection method,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IET Image Processing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 10, no. 1, pp. 16-23, 2016.</span>
            </p>
          </li>
        
          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Zengfu Wang,
                Dense SIFT for ghost-free multi-exposure fusion,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Journal of Visual Communication and Image Representation</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 31, pp. 208-224, 2015.</span>
              (<a class="XqQF9c" href="https://github.com/yuliu316316/DSIFT-EF" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Shuping Liu, Zengfu Wang,
                A general framework for image fusion based on multi-scale transform and sparse representation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Information Fusion</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 24, pp. 147-164, 2015.</span>
              (<strong>ESI Hot Paper, ESI Highly Cited Paper, Google Scholar Citations: 1600+</strong>)
              (<a class="XqQF9c" href="https://github.com/yuliu316316/MST-SR-Fusion-Toolbox" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>MST-SR-Fusion-Toolbox</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Zengfu Wang,
                Simultaneous image fusion and denoising with adaptive sparse representation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                IET Image Processing</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 9, no. 5, pp. 347-357, 2015.</span>
              (<strong>2017 Premium Award for the Best Paper in IET Image Processing, ESI Highly Cited Paper, Google Scholar Citations: 400+</strong>)
              (<a class="XqQF9c" href="https://github.com/yuliu316316/ASR-Fusion" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Shuping Liu, Zengfu Wang,
                Multi-focus image fusion with dense SIFT,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Information Fusion</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 23, pp. 139-155, 2015.</span>
              (<strong>ESI Highly Cited Paper, Google Scholar Citations: 500+</strong>)
              (<a class="XqQF9c" href="https://github.com/yuliu316316/DSIFT-MFIF" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Baocai Yin, Jun Yu, Zengfu Wang,
                Cross-level: A practical strategy for convolutional neural networks based image classification,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                1st CCF Chinese Conference on Computer Vision (CCCV)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, Xi'an, China, Sep. 18-20, 2015, CCIS 546, pp. 398-406.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Shuping Liu, <strong>Yu Liu</strong>, Jun Yu, Zengfu Wang,
                Hierarchical static hand gesture recognition by combining finger detection and HOG features,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Journal of Image and Graphics</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 20, no. 6, pp. 781-788, 2015.</span>
              (In Chinese)
              (<strong>2016 Journal of Image and Graphics Excellent Paper</strong>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Shuping Liu, Yang Cao, Zengfu Wang,
                A practical algorithm for automatic chessboard corner detection,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                21th IEEE International Conference on Image Processing (ICIP)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, Paris, France, Oct. 27-30, 2014, pp. 3394-3398.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Baocai Yin, <strong>Yu Liu</strong>, Zengfu Wang,
                Moving shadow detection by combining chromaticity and texture invariance,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Journal of Image and Graphics</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 19, no. 6, pp. 896-905, 2014.</span>
              (In Chinese)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Wenhui Xiang, <strong>Yu Liu</strong>, Yang Cao, Zengfu Wang,
                3D Ground Plane Estimation from a Monocular Vehicle-borne Image,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Robot</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 36, no. 1, pp. 76-82, 2014.</span>
              (In Chinese)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Shuping Liu, Zengfu Wang,
                Medical Image Fusion by combining nonsubsampled contourlet transform and sparse representation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                6th Chinese Conference on Pattern Recognition (CCPR)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, Changsha, China, Nov. 17-19, 2014, pp. 372-381.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;">Shuping Liu, <strong>Yu Liu</strong>, Jun Yu, Zengfu Wang,
                A static hand gesture recognition algorithm based on Krawtchouk moments,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                6th Chinese Conference on Pattern Recognition (CCPR)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, Changsha, China, Nov. 17-19, 2014, pp. 321-330.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Zengfu Wang,
                Multi-focus image fusion based on wavelet transform and adaptive block,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                Journal of Image and Graphics</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, vol. 18, no. 11, pp. 1435-1444, 2013.</span>
              (In Chinese)
              (<a class="XqQF9c" href="https://github.com/yuliu316316/DWTDE-Fusion" target="_blank" style="color: inherit; text-decoration: none;"><font color="#034EA1"><strong>Code</strong></font></a>)
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Zengfu Wang,
                A practical pan-sharpening method with wavelet transform and sparse representation,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                10th IEEE International Conference on Imaging Systems and Techniques (IST)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, Beijing, China, Oct. 22-23, 2013, pp. 288-293.</span>
            </p>
          </li>

          <li dir="ltr" class="zfr3Q TYR86d lsiHE " style="margin-left: 15.0pt;">
            <p dir="ltr" role="presentation" class="zfr3Q CDt4Ke "
              style="background-color: transparent; border-bottom: none; border-left: none; border-right: none; border-top: none; line-height: 1.38; margin-bottom: 0.0pt; margin-left: 0.0pt; margin-right: 4.0pt; margin-top: 0.0pt; padding-bottom: 0.0pt; padding-left: 0.0pt; padding-right: 0.0pt; padding-top: 0.0pt; text-align: justify; text-indent: 0.0pt;">
              <span class="C9DxTc " style="color: #000000; font-variant: normal;"><strong>Yu Liu</strong>, Zengfu Wang,
                Multi-focus image fusion based on sparse representation with adaptive sparse domain selection,
              </span><span class="C9DxTc " style="color: #000000; font-style: italic; font-variant: normal;">
                7th International Conference on Image and Graphics (ICIG)</span><span class="C9DxTc " style="color: #000000; font-variant: normal;">, Qingdao, China, Jul. 26-28, 2013, pp. 591-596.</span>
            </p>
          </li>





        </ol>




        <!-- <ul class="graid3-ul">
        <div style="text-align: justify; display: block; margin-right: auto;">
	Here are the Impact Factors (IF) of selected journals of my publications. <br>
        - IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI): 20.8 <br>
	- International Journal of Computer Vision (IJCV): 11.6 <br>
	- IEEE Transactions on Industrial Informatics (TII): 11.7<br>
	- IEEE Transactions on Image Processing (TIP): 10.8 <br>
	- IEEE Transactions on Neural Networks and Learning Systems (TNNLS): 10.2<br>
	- IEEE Transactions on Cybernetics (TCyb): 9.4 <br>
	- IEEE Transactions on Intelligent Transportation Systems (TITS): 7.9  <br>
	- IEEE Transactions on Multimedia (TMM): 8.4 <br> 
	- IEEE Transactions on Geoscience and Remote Sensing (TGRS): 7.5 <br>
	- IEEE Transactions on Circuits and Systems for Video Technology (TCSVT): 8.3 <br>
	- IEEE Transactions on Automation Science and Engineering (TASE): 5.9 <br>
	- IEEE Transactions on Instrumentation and Measurement (TIM): 5.6 <br>
	- IEEE Transactions on Computational Imaging (TCI): 4.2 <br>
	- IEEE Transactions on Consumer Electronics (TCE): 4.3 <br>
	- IEEE Transactions on Emerging Topics in Computational Intelligence (TETCI): 4.851 <br>
	- IEEE Journal of Biomedical and Health Informatics (JBHI): 7.021<br>
	- SCIENCE CHINA Information Sciences (SCIS): 7.275<br>
	- ACM Transactions on Multimedia Computing Communications and Applications (TOMM): 4.094<br>
	- Information Sciences: 8.233 <br>
	- Neurocomputing: 5.779 <br>
	- Pattern Recognition Letters (PRL): 4.757 <br>
	- IEEE Signal Processing Letters (SPL): 3.201 <br>
	- International Journal of Remote Sensing (IJRS): 3.531	 <br>
	- Signal Processing: Image Communication (SPIC): 3.453 <br>
	- Multimedia Tools and Applications (MTAP): 2.577 <br>
	- Journal of Electronic Imaging (JEI): 0.829<br> -->




      </div>
      </ul>
    </div>
  </div>

  <script type="text/javascript" src="./PUBLICATIONS_files/jquery.min.js.下载"></script>
  <script type="text/javascript" src="./PUBLICATIONS_files/bootstrap.js.下载"></script>
  <script type="text/javascript" src="./PUBLICATIONS_files/jquery.banner.js.下载"></script>
  <script type="text/javascript" src="./PUBLICATIONS_files/jquery.prettyPhoto.js.下载"></script>
  <script type="text/javascript" src="./PUBLICATIONS_files/jquery.isotope.js.下载"></script>
  <script type="text/javascript" src="./PUBLICATIONS_files/main.js.下载"></script>




  </div>
</body>

</html>
